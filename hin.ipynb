{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "path = '/content/drive/MyDrive/HTIC - Assignments/Dataset/hin/hin_train.json'  # adjust as needed\n",
        "data = []\n",
        "\n",
        "# Read line by line\n",
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Check a few examples\n",
        "print(len(data))\n",
        "print(data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPYEjULWgRNd",
        "outputId": "10464c29-bb76-4ac7-d209-2cb930064a59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1299155\n",
            "{'unique_identifier': 'hin1', 'native word': 'जन्मदिवस', 'english word': 'janamdivas', 'source': 'Dakshina', 'score': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_vocab(pairs):\n",
        "    src_chars = Counter()  # Romanized (English letters)\n",
        "    tgt_chars = Counter()  # Devanagari\n",
        "    for p in pairs:\n",
        "        src_chars.update(list(p[\"english word\"]))\n",
        "        tgt_chars.update(list(p[\"native word\"]))\n",
        "\n",
        "    src_vocab = ['<PAD>', '<SOS>', '<EOS>'] + sorted(src_chars)\n",
        "    tgt_vocab = ['<PAD>', '<SOS>', '<EOS>'] + sorted(tgt_chars)\n",
        "\n",
        "    src_to_ix = {ch: i for i, ch in enumerate(src_vocab)}\n",
        "    tgt_to_ix = {ch: i for i, ch in enumerate(tgt_vocab)}\n",
        "    ix_to_tgt = {i: ch for ch, i in tgt_to_ix.items()}\n",
        "\n",
        "    return src_to_ix, tgt_to_ix, ix_to_tgt\n",
        "\n",
        "src_to_ix, tgt_to_ix, ix_to_tgt = build_vocab(data)\n",
        "\n",
        "print(f\"Source vocab size: {len(src_to_ix)}\")\n",
        "print(f\"Target vocab size: {len(tgt_to_ix)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSeXc-NYgWzH",
        "outputId": "c91d8902-8be8-4ded-f5c5-d9b866feb6a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source vocab size: 29\n",
            "Target vocab size: 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sequence(text, mapping):\n",
        "    return [mapping['<SOS>']] + [mapping[ch] for ch in text if ch in mapping] + [mapping['<EOS>']]\n",
        "\n",
        "encoded_data = [\n",
        "    (encode_sequence(d[\"english word\"], src_to_ix),\n",
        "     encode_sequence(d[\"native word\"], tgt_to_ix))\n",
        "    for d in data\n",
        "]\n",
        "\n",
        "print(f\"Encoded {len(encoded_data)} pairs\")\n",
        "print(\"Example input encoding:\", encoded_data[0][0])\n",
        "print(\"Example target encoding:\", encoded_data[0][1])\n"
      ],
      "metadata": {
        "id": "MPY5fYvPhqX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f178810a-54a0-45df-8714-c92e361139d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 1299155 pairs\n",
            "Example input encoding: [1, 12, 3, 16, 3, 15, 6, 11, 24, 3, 21, 2]\n",
            "Example target encoding: [1, 26, 38, 68, 43, 36, 56, 48, 51, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# ---- Step 1: Pad and prepare dataset ----\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, data_pairs):\n",
        "        self.data_pairs = data_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_seq, tgt_seq = self.data_pairs[idx]\n",
        "        return torch.tensor(src_seq, dtype=torch.long), torch.tensor(tgt_seq, dtype=torch.long)\n",
        "\n",
        "# Collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_batch = pad_sequence(src_batch, padding_value=src_to_ix['<PAD>'], batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_to_ix['<PAD>'], batch_first=True)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# Use a small sample for quick testing (you can increase later)\n",
        "sampled_data = encoded_data[:5000]\n",
        "\n",
        "dataset = TransliterationDataset(sampled_data)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "for src, tgt in dataloader:\n",
        "    print(\"Source batch shape:\", src.shape)\n",
        "    print(\"Target batch shape:\", tgt.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcB5zHu9h_bG",
        "outputId": "33e549cd-7d72-4d8e-c87c-1792471b60e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Source batch shape: torch.Size([32, 16])\n",
            "Target batch shape: torch.Size([32, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, cell_type='gru'):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        if cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, cell_type='gru'):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        if cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_token, hidden):\n",
        "        embedded = self.embedding(input_token.unsqueeze(1))\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "        hidden = self.encoder(src)\n",
        "        input_token = tgt[:, 0]  # <SOS> token\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden = self.decoder(input_token, hidden)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input_token = tgt[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "hqiMQQUqiE2M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "input_dim = len(src_to_ix)\n",
        "output_dim = len(tgt_to_ix)\n",
        "embed_dim = 256\n",
        "hidden_dim = 512\n",
        "\n",
        "encoder = EncoderRNN(input_dim, embed_dim, hidden_dim)\n",
        "decoder = DecoderRNN(output_dim, embed_dim, hidden_dim)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_to_ix['<PAD>'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "sampled_data = encoded_data[:30000]\n",
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        loss = criterion(output[:, 1:].reshape(-1, output_dim), tgt[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEuOeEH_pbXG",
        "outputId": "991b4626-e484-4872-d90a-64f7f1fe519c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 2.7156\n",
            "Epoch [2/20], Loss: 1.6927\n",
            "Epoch [3/20], Loss: 1.2587\n",
            "Epoch [4/20], Loss: 0.9923\n",
            "Epoch [5/20], Loss: 0.8100\n",
            "Epoch [6/20], Loss: 0.6634\n",
            "Epoch [7/20], Loss: 0.4992\n",
            "Epoch [8/20], Loss: 0.3933\n",
            "Epoch [9/20], Loss: 0.3069\n",
            "Epoch [10/20], Loss: 0.2259\n",
            "Epoch [11/20], Loss: 0.1425\n",
            "Epoch [12/20], Loss: 0.0937\n",
            "Epoch [13/20], Loss: 0.0619\n",
            "Epoch [14/20], Loss: 0.0502\n",
            "Epoch [15/20], Loss: 0.0409\n",
            "Epoch [16/20], Loss: 0.0311\n",
            "Epoch [17/20], Loss: 0.0268\n",
            "Epoch [18/20], Loss: 0.0292\n",
            "Epoch [19/20], Loss: 0.0414\n",
            "Epoch [20/20], Loss: 0.1212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(seq, ix_to_char):\n",
        "    return ''.join([ix_to_char[i] for i in seq if i not in [tgt_to_ix['<PAD>'], tgt_to_ix['<SOS>'], tgt_to_ix['<EOS>']]])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
        "        preds = output.argmax(2)\n",
        "        print(\"Romanized:\", decode_sequence(src[0].cpu().numpy(), {v: k for k, v in src_to_ix.items()}))\n",
        "        print(\"Expected:\", decode_sequence(tgt[0].cpu().numpy(), ix_to_tgt))\n",
        "        print(\"Predicted:\", decode_sequence(preds[0].cpu().numpy(), ix_to_tgt))\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G93DkVUHiNDH",
        "outputId": "4ca27ad2-cd33-49e3-86da-858744affc4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Romanized: ganwai\n",
            "Expected: गंवाई\n",
            "Predicted: गंवाई\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 8: Interactive prediction ---\n",
        "def transliterate_word(model, word, src_to_ix, tgt_to_ix, ix_to_tgt, device, max_len=30):\n",
        "    model.eval()\n",
        "\n",
        "    # Encode input sequence\n",
        "    input_seq = [src_to_ix['<SOS>']] + [src_to_ix[ch] for ch in word if ch in src_to_ix] + [src_to_ix['<EOS>']]\n",
        "    input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode\n",
        "    hidden = model.encoder(input_tensor)\n",
        "\n",
        "    # Start decoding\n",
        "    input_token = torch.tensor([tgt_to_ix['<SOS>']], dtype=torch.long).to(device)\n",
        "    decoded_chars = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        output, hidden = model.decoder(input_token, hidden)\n",
        "        pred_token = output.argmax(1).item()\n",
        "\n",
        "        if pred_token == tgt_to_ix['<EOS>']:\n",
        "            break\n",
        "\n",
        "        decoded_chars.append(ix_to_tgt[pred_token])\n",
        "        input_token = torch.tensor([pred_token], dtype=torch.long).to(device)\n",
        "\n",
        "    return ''.join(decoded_chars)\n",
        "\n",
        "# Test manually\n",
        "test_words = [\"ghar\", \"janamdivas\", \"pustak\", \"ladka\", \"dil\"]\n",
        "for w in test_words:\n",
        "    print(f\"{w} → {transliterate_word(model, w, src_to_ix, tgt_to_ix, ix_to_tgt, device)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5-sPKi-ikYA",
        "outputId": "fba92cf9-b538-4706-b969-b1160f5f1b15"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ghar → घार\n",
            "janamdivas → जन्दिमिस\n",
            "pustak → पुस्टका\n",
            "ladka → लक्ड़ा\n",
            "dil → दिल\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"Enter a Romanized Hindi word (or 'quit' to stop): \").strip()\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "    result = transliterate_word(model, user_input, src_to_ix, tgt_to_ix, ix_to_tgt, device)\n",
        "    print(f\"🔤 {user_input} → {result}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJMTtozwipUW",
        "outputId": "ea6f1745-c824-42cb-fa1e-5432e10eb4de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a Romanized Hindi word (or 'quit' to stop): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Create reverse vocab for target\n",
        "tgt_vocab = type('', (), {})()  # simple object with .itos attribute\n",
        "tgt_vocab.itos = ix_to_tgt\n",
        "\n",
        "# ✅ Accuracy Evaluation Function\n",
        "def evaluate_accuracy(model, dataloader, tgt_vocab, device='cuda'):\n",
        "    model.eval()\n",
        "    total_chars = 0\n",
        "    correct_chars = 0\n",
        "    total_words = 0\n",
        "    correct_words = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            outputs = model(src, tgt, teacher_forcing_ratio=0.0)\n",
        "            preds = outputs.argmax(2)\n",
        "\n",
        "            for i in range(len(preds)):\n",
        "                pred_seq = [tgt_vocab.itos[idx] for idx in preds[i].cpu().numpy()]\n",
        "                true_seq = [tgt_vocab.itos[idx] for idx in tgt[i].cpu().numpy()]\n",
        "\n",
        "                # Remove special tokens\n",
        "                for tok in ['<PAD>', '<SOS>', '<EOS>']:\n",
        "                    if tok in pred_seq:\n",
        "                        pred_seq = pred_seq[:pred_seq.index(tok)]\n",
        "                    if tok in true_seq:\n",
        "                        true_seq = true_seq[:true_seq.index(tok)]\n",
        "\n",
        "                # --- Character-level accuracy ---\n",
        "                min_len = min(len(pred_seq), len(true_seq))\n",
        "                correct_chars += sum(1 for a, b in zip(pred_seq[:min_len], true_seq[:min_len]) if a == b)\n",
        "                total_chars += max(len(pred_seq), len(true_seq))\n",
        "\n",
        "                # --- Word-level accuracy ---\n",
        "                if ''.join(pred_seq) == ''.join(true_seq):\n",
        "                    correct_words += 1\n",
        "                total_words += 1\n",
        "\n",
        "    char_acc = correct_chars / total_chars if total_chars > 0 else 0\n",
        "    word_acc = correct_words / total_words if total_words > 0 else 0\n",
        "\n",
        "    print(f\"\\n✅ Character Accuracy: {char_acc * 100:.2f}%\")\n",
        "    print(f\"✅ Word Accuracy: {word_acc * 100:.2f}%\")\n",
        "\n",
        "    return char_acc, word_acc\n",
        "\n",
        "\n",
        "# ✅ Call the function\n",
        "char_acc, word_acc = evaluate_accuracy(model, dataloader, tgt_vocab, device=device)\n"
      ],
      "metadata": {
        "id": "-RVsBSV2W3cI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003f9e2c-f81c-444e-f224-55a22d1b9c5f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:10<00:00, 15.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Character Accuracy: 0.00%\n",
            "✅ Word Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ggUc_atOVTe"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}