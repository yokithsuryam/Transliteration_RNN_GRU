{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dec0e247-e084-41c8-9fbb-5823bf3d5136",
      "metadata": {
        "id": "dec0e247-e084-41c8-9fbb-5823bf3d5136"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "\n",
        "# ================================\n",
        "# 1. Configuration\n",
        "# ================================\n",
        "# --- Hyperparameters ---\n",
        "EMBED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "N_EPOCHS = 10  # 100 epochs is long, let's start with 10\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# --- Special Tokens ---\n",
        "PAD_TOKEN = '<PAD>'\n",
        "SOS_TOKEN = '<SOS>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "\n",
        "# --- Device Setup ---\n",
        "# This will automatically use your RTX 3050 if available\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d0bc674-e001-48f4-baf6-9431be6a9f99",
      "metadata": {
        "id": "1d0bc674-e001-48f4-baf6-9431be6a9f99"
      },
      "outputs": [],
      "source": [
        "def load_local_csv(path):\n",
        "    \"\"\"Loads data from a local tab-separated .csv file.\"\"\"\n",
        "    data = []\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) == 2:\n",
        "                    # Match the dict structure of the Colab example\n",
        "                    data.append({\n",
        "                        \"english word\": parts[0],\n",
        "                        \"native word\": parts[1]\n",
        "                    })\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def build_vocab(pairs):\n",
        "    \"\"\"Builds source and target vocabularies from the data.\"\"\"\n",
        "    src_chars = Counter()\n",
        "    tgt_chars = Counter()\n",
        "    for p in pairs:\n",
        "        src_chars.update(list(p[\"english word\"]))\n",
        "        tgt_chars.update(list(p[\"native word\"]))\n",
        "\n",
        "    src_vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN] + sorted(src_chars)\n",
        "    tgt_vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN] + sorted(tgt_chars)\n",
        "\n",
        "    src_to_ix = {ch: i for i, ch in enumerate(src_vocab)}\n",
        "    tgt_to_ix = {ch: i for i, ch in enumerate(tgt_vocab)}\n",
        "    ix_to_tgt = {i: ch for ch, i in tgt_to_ix.items()}\n",
        "\n",
        "    return src_to_ix, tgt_to_ix, ix_to_tgt\n",
        "\n",
        "def encode_sequence(text, mapping):\n",
        "    \"\"\"Converts a text string to a list of token indices.\"\"\"\n",
        "    return [mapping[SOS_TOKEN]] + \\\n",
        "           [mapping[ch] for ch in text if ch in mapping] + \\\n",
        "           [mapping[EOS_TOKEN]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1848eed8-7070-4a5e-a0ed-571012e85a55",
      "metadata": {
        "id": "1848eed8-7070-4a5e-a0ed-571012e85a55"
      },
      "outputs": [],
      "source": [
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, data_pairs):\n",
        "        self.data_pairs = data_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_seq, tgt_seq = self.data_pairs[idx]\n",
        "        return torch.tensor(src_seq, dtype=torch.long), \\\n",
        "               torch.tensor(tgt_seq, dtype=torch.long)\n",
        "\n",
        "def collate_fn_factory(pad_idx_src, pad_idx_tgt):\n",
        "    \"\"\"Creates a collate_fn to pad batches.\"\"\"\n",
        "    def collate_fn(batch):\n",
        "        src_batch, tgt_batch = zip(*batch)\n",
        "        src_batch = pad_sequence(src_batch, padding_value=pad_idx_src, batch_first=True)\n",
        "        tgt_batch = pad_sequence(tgt_batch, padding_value=pad_idx_tgt, batch_first=True)\n",
        "        return src_batch, tgt_batch\n",
        "    return collate_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b5bca38d-6b27-4bb0-8a2c-c429cead7153",
      "metadata": {
        "id": "b5bca38d-6b27-4bb0-8a2c-c429cead7153"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        # Using GRU as in the simple example\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [batch_size, src_len]\n",
        "        embedded = self.embedding(src)\n",
        "        # embedded = [batch_size, src_len, embed_dim]\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # outputs = [batch_size, src_len, hidden_dim]\n",
        "        # hidden = [1, batch_size, hidden_dim]\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_token, hidden):\n",
        "        # input_token = [batch_size]\n",
        "        # hidden = [1, batch_size, hidden_dim]\n",
        "        embedded = self.embedding(input_token.unsqueeze(1))\n",
        "        # embedded = [batch_size, 1, embed_dim]\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        # output = [batch_size, 1, hidden_dim]\n",
        "        # hidden = [1, batch_size, hidden_dim]\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        # prediction = [batch_size, output_dim]\n",
        "        return prediction, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "        hidden = self.encoder(src)\n",
        "        input_token = tgt[:, 0]  # <SOS> token\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden = self.decoder(input_token, hidden)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input_token = tgt[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bef8dbb9-9074-49e5-90ae-09a7d32083ed",
      "metadata": {
        "id": "bef8dbb9-9074-49e5-90ae-09a7d32083ed"
      },
      "outputs": [],
      "source": [
        "def transliterate_word(model, word, src_to_ix, tgt_to_ix, ix_to_tgt, device, max_len=30):\n",
        "    model.eval()\n",
        "\n",
        "    # Encode input sequence\n",
        "    input_seq = [src_to_ix[SOS_TOKEN]] + \\\n",
        "                [src_to_ix[ch] for ch in word if ch in src_to_ix] + \\\n",
        "                [src_to_ix[EOS_TOKEN]]\n",
        "    input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(input_tensor)\n",
        "\n",
        "    # Start decoding\n",
        "    input_token = torch.tensor([tgt_to_ix[SOS_TOKEN]], dtype=torch.long).to(device)\n",
        "    decoded_chars = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(input_token, hidden)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "\n",
        "        if pred_token == tgt_to_ix[EOS_TOKEN]:\n",
        "            break\n",
        "\n",
        "        decoded_chars.append(ix_to_tgt[pred_token])\n",
        "        input_token = torch.tensor([pred_token], dtype=torch.long).to(device)\n",
        "\n",
        "    return ''.join(decoded_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a11d143d-e469-4e50-b007-21d53c1c9675",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a11d143d-e469-4e50-b007-21d53c1c9675",
        "outputId": "081c1ebc-31ec-477d-8a88-bbcf625442e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on device: cuda\n",
            "\n",
            "Loaded 51200 training pairs.\n",
            "Loaded 4096 validation pairs.\n",
            "Source vocab size: 29\n",
            "Target vocab size: 67\n",
            "\n",
            "Starting training for 10 epochs...\n",
            "Epoch [1/10], Train Loss: 1.4548, Valid Loss: 1.2972\n",
            "Epoch [2/10], Train Loss: 0.9357, Valid Loss: 1.2182\n",
            "Epoch [3/10], Train Loss: 0.8305, Valid Loss: 1.1827\n",
            "Epoch [4/10], Train Loss: 0.7740, Valid Loss: 1.1782\n",
            "Epoch [5/10], Train Loss: 0.7242, Valid Loss: 1.1780\n",
            "Epoch [6/10], Train Loss: 0.6935, Valid Loss: 1.1651\n",
            "Epoch [7/10], Train Loss: 0.6692, Valid Loss: 1.1898\n",
            "Epoch [8/10], Train Loss: 0.6442, Valid Loss: 1.2199\n",
            "Epoch [9/10], Train Loss: 0.6235, Valid Loss: 1.2006\n",
            "Epoch [10/10], Train Loss: 0.6087, Valid Loss: 1.1955\n",
            "...Training complete!\n",
            "\n",
            "--- Interactive Transliteration (Hindi) ---\n",
            "Type a Romanized word and press Enter (or 'quit' to stop).\n",
            "ðŸ”¤ > ghar\n",
            "   â†’ à¤˜à¤°\n",
            "\n",
            "ðŸ”¤ > ajanabee\n",
            "   â†’ à¤…à¤œà¤¨à¤¬à¥€\n",
            "\n",
            "ðŸ”¤ > ajanabee,à¤…à¤œà¤¨à¤¬à¥€\n",
            "   â†’ à¤…à¤œà¤¨à¤¬à¥€\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 127\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mType a Romanized word and press Enter (or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to stop).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mðŸ”¤ > \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_input.lower() == \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    129\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mW:\\Anaconda\\envs\\my_env\\Lib\\site-packages\\ipykernel\\kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mW:\\Anaconda\\envs\\my_env\\Lib\\site-packages\\ipykernel\\kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# 5.5 New Evaluate Function\n",
        "# ================================\n",
        "# Add this new function right before your \"Main Execution\" section\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Runs the model on the validation dataset.\"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad(): # No gradients needed\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            # Forward pass (no teacher forcing)\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
        "\n",
        "            # Reshape for loss\n",
        "            output_dim = output.shape[-1]\n",
        "            loss = criterion(\n",
        "                output[:, 1:].reshape(-1, output_dim),\n",
        "                tgt[:, 1:].reshape(-1)\n",
        "            )\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. Main Execution (Rewritten for 'hin' dataset)\n",
        "# ================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Running on device: {DEVICE}\")\n",
        "\n",
        "    # --- Step 1: Define file paths ---\n",
        "    # We hardcode the paths to the 'hin' dataset\n",
        "    # Based on your previous path, assuming 'aksharantar_sampled' is the root\n",
        "    DATA_ROOT = r\"W:\\CV\\HTIC\\Qns-3\\hin\"\n",
        "    train_file = os.path.join(DATA_ROOT, \"hin_train.txt\")\n",
        "    valid_file = os.path.join(DATA_ROOT, \"hin_valid.txt\")\n",
        "    # test_file = os.path.join(DATA_ROOT, \"hin_test.csv\") # We'll use valid for now\n",
        "\n",
        "    # --- Step 2: Load Data ---\n",
        "    # We use the same load_local_csv function from before\n",
        "    train_data = load_local_csv(train_file)\n",
        "    valid_data = load_local_csv(valid_file)\n",
        "\n",
        "    if train_data and valid_data:\n",
        "        print(f\"\\nLoaded {len(train_data)} training pairs.\")\n",
        "        print(f\"Loaded {len(valid_data)} validation pairs.\")\n",
        "\n",
        "        # --- Step 3: Build Vocabs (from training data ONLY) ---\n",
        "        src_to_ix, tgt_to_ix, ix_to_tgt = build_vocab(train_data)\n",
        "        print(f\"Source vocab size: {len(src_to_ix)}\")\n",
        "        print(f\"Target vocab size: {len(tgt_to_ix)}\")\n",
        "\n",
        "        # --- Step 4: Encode Data ---\n",
        "        encoded_train_data = [\n",
        "            (encode_sequence(d[\"english word\"], src_to_ix),\n",
        "             encode_sequence(d[\"native word\"], tgt_to_ix))\n",
        "            for d in train_data\n",
        "        ]\n",
        "        encoded_valid_data = [\n",
        "            (encode_sequence(d[\"english word\"], src_to_ix),\n",
        "             encode_sequence(d[\"native word\"], tgt_to_ix))\n",
        "            # Filter out words with chars not in training vocab\n",
        "            for d in valid_data\n",
        "            if all(c in src_to_ix for c in d[\"english word\"]) and \\\n",
        "               all(c in tgt_to_ix for c in d[\"native word\"])\n",
        "        ]\n",
        "\n",
        "        # --- Step 5: Create DataLoaders ---\n",
        "        pad_src_idx = src_to_ix[PAD_TOKEN]\n",
        "        pad_tgt_idx = tgt_to_ix[PAD_TOKEN]\n",
        "        my_collate_fn = collate_fn_factory(pad_src_idx, pad_tgt_idx)\n",
        "\n",
        "        train_dataset = TransliterationDataset(encoded_train_data)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate_fn)\n",
        "\n",
        "        valid_dataset = TransliterationDataset(encoded_valid_data)\n",
        "        valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=my_collate_fn)\n",
        "\n",
        "        # --- Step 6: Initialize Model ---\n",
        "        input_dim = len(src_to_ix)\n",
        "        output_dim = len(tgt_to_ix)\n",
        "\n",
        "        encoder = EncoderRNN(input_dim, EMBED_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "        decoder = DecoderRNN(output_dim, EMBED_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "        model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=pad_tgt_idx)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        # --- Step 7: Training Loop ---\n",
        "        print(f\"\\nStarting training for {N_EPOCHS} epochs...\")\n",
        "        for epoch in range(N_EPOCHS):\n",
        "            model.train() # Set to train mode\n",
        "            total_loss = 0\n",
        "\n",
        "            for src, tgt in train_dataloader:\n",
        "                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(src, tgt, teacher_forcing_ratio=0.5) # Use teacher forcing\n",
        "\n",
        "                output_dim_loss = output.shape[-1]\n",
        "                loss = criterion(\n",
        "                    output[:, 1:].reshape(-1, output_dim_loss),\n",
        "                    tgt[:, 1:].reshape(-1)\n",
        "                )\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            # --- Evaluate on validation data ---\n",
        "            valid_loss = evaluate(model, valid_dataloader, criterion, DEVICE)\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{N_EPOCHS}], Train Loss: {total_loss/len(train_dataloader):.4f}, Valid Loss: {valid_loss:.4f}\")\n",
        "\n",
        "        print(\"...Training complete!\")\n",
        "\n",
        "        # --- Step 8: Interactive Prediction Loop ---\n",
        "        print(\"\\n--- Interactive Transliteration (Hindi) ---\")\n",
        "        print(\"Type a Romanized word and press Enter (or 'quit' to stop).\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"ðŸ”¤ > \").strip()\n",
        "            if user_input.lower() == \"quit\":\n",
        "                break\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            result = transliterate_word(model, user_input, src_to_ix, tgt_to_ix, ix_to_tgt, DEVICE)\n",
        "            print(f\"   â†’ {result}\\n\")\n",
        "\n",
        "    else:\n",
        "        print(\"Could not start. Make sure these files exist:\")\n",
        "        print(f\"  {train_file}\")\n",
        "        print(f\"  {valid_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3f4dbdb9-00f0-4487-b28c-6b15a3cf0421",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f4dbdb9-00f0-4487-b28c-6b15a3cf0421",
        "outputId": "9e667b0d-ad29-4a6c-936e-c5130808d995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model saved to W:\\CV\\HTIC\\Qns-3\\hin\\hin_model.pth\n",
            "\n",
            "Running validation on the test set...\n",
            "\n",
            "Running validation on the test set...\n",
            "Collecting editdistance\n",
            "  Downloading editdistance-0.8.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
            "Downloading editdistance-0.8.1-cp311-cp311-win_amd64.whl (79 kB)\n",
            "Installing collected packages: editdistance\n",
            "Successfully installed editdistance-0.8.1\n",
            "Loaded 4094 test pairs (filtered from 4096).\n",
            "\n",
            "--- Test Set Results ---\n",
            "âœ… Exact Match Accuracy: 26.50% (1085 / 4094)\n",
            "ðŸ“Š Character Error Rate (CER): 23.67% (Lower is better)\n",
            "\n",
            "--- Interactive Transliteration (Hindi) ---\n"
          ]
        }
      ],
      "source": [
        "# --- SAVE THE MODEL ---\n",
        "# This saves the model's learned weights to a file\n",
        "MODEL_SAVE_PATH = r\"W:\\CV\\HTIC\\Qns-3\\hin\\hin_model.pth\"\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"âœ… Model saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "# --- Step 8: Run Validation on Test Set ---\n",
        "print(\"\\nRunning validation on the test set...\")\n",
        "# (The rest of your script...)\n",
        "\n",
        "# ================================\n",
        "# NEW: Step 8: Run Validation on Test Set\n",
        "# ================================\n",
        "print(\"\\nRunning validation on the test set...\")\n",
        "!pip install editdistance\n",
        "try:\n",
        "    import editdistance\n",
        "except ImportError:\n",
        "    print(\"---\")\n",
        "    print(\"To calculate Character Error Rate (CER), please install 'editdistance'\")\n",
        "    print(\"Run this in your (torch_gpu) terminal: pip install editdistance\")\n",
        "    print(\"---\")\n",
        "    editdistance = None\n",
        "\n",
        "# 1. Load test data\n",
        "test_file = os.path.join(DATA_ROOT, \"hin_test.txt\")\n",
        "test_data = load_local_csv(test_file)\n",
        "\n",
        "if test_data:\n",
        "    # Filter test data to only include characters seen during training\n",
        "    filtered_test_data = []\n",
        "    for d in test_data:\n",
        "        if all(c in src_to_ix for c in d[\"english word\"]) and \\\n",
        "            all(c in tgt_to_ix for c in d[\"native word\"]):\n",
        "            filtered_test_data.append(d)\n",
        "\n",
        "    print(f\"Loaded {len(filtered_test_data)} test pairs (filtered from {len(test_data)}).\")\n",
        "\n",
        "    correct_count = 0\n",
        "    total_count = len(filtered_test_data)\n",
        "    total_edit_distance = 0\n",
        "    total_target_chars = 0\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # Loop through every single word in the test set\n",
        "    for pair in filtered_test_data:\n",
        "        source_word = pair[\"english word\"]\n",
        "        target_word = pair[\"native word\"]\n",
        "\n",
        "        # Use your existing function to get the prediction\n",
        "        predicted_word = transliterate_word(\n",
        "            model, source_word, src_to_ix, tgt_to_ix, ix_to_tgt, DEVICE\n",
        "        )\n",
        "\n",
        "        # 1. Check for exact match\n",
        "        if predicted_word == target_word:\n",
        "            correct_count += 1\n",
        "\n",
        "        # 2. Calculate edit distance (if library is installed)\n",
        "        if editdistance:\n",
        "            total_edit_distance += editdistance.eval(predicted_word, target_word)\n",
        "            total_target_chars += len(target_word)\n",
        "\n",
        "    # --- Print Final Results ---\n",
        "    accuracy = (correct_count / total_count) * 100\n",
        "    print(f\"\\n--- Test Set Results ---\")\n",
        "    print(f\"âœ… Exact Match Accuracy: {accuracy:.2f}% ({correct_count} / {total_count})\")\n",
        "\n",
        "    if editdistance and total_target_chars > 0:\n",
        "        cer = (total_edit_distance / total_target_chars) * 100\n",
        "        print(f\"ðŸ“Š Character Error Rate (CER): {cer:.2f}% (Lower is better)\")\n",
        "\n",
        "else:\n",
        "    print(\"Could not find test file to run validation.\")\n",
        "\n",
        "# ================================\n",
        "# OLD: Step 9: Interactive Prediction Loop (was Step 8)\n",
        "# ================================\n",
        "print(\"\\n--- Interactive Transliteration (Hindi) ---\")\n",
        "# (Your existing interactive loop code goes here...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "8U4Vj1K0MVEQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U4Vj1K0MVEQ",
        "outputId": "701fd2cd-5655-4af3-ae9b-fa4a888c53f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing 'editdistance' for CER calculation...\n",
            "Installation complete.\n",
            "\n",
            "Running validation on the test set...\n",
            "Loaded 4094 test pairs (filtered from 4096).\n",
            "\n",
            "--- Test Set Results ---\n",
            "âœ… Exact Match Accuracy: 26.50%\n",
            "ðŸ“Š Character Error Rate (CER): 23.67%\n",
            "\n",
            "âœ… Model weights saved to W:\\CV\\HTIC\\Qns-3\\hin\\hin_model.pth\n",
            "\n",
            "==================================================================\n",
            "      Dynamic Model Analysis (for 1-Layer GRU)\n",
            "==================================================================\n",
            "\n",
            "--- Model Configuration ---\n",
            "  - Cell Type: GRU (hardcoded in notebook)\n",
            "  - Num Layers (n): 1 (hardcoded in notebook)\n",
            "  - Embedding Dim (e): 256\n",
            "  - Hidden Dim (h): 512\n",
            "  - Source Vocab (V_src): 29\n",
            "  - Target Vocab (V_tgt): 67\n",
            "\n",
            "--- 1. Practical Parameter Calculation (from Config) ---\n",
            "  1. Encoder Embedding (V_src * e):           29 * 256 =      7,424\n",
            "  2. Encoder GRU (n=1):                         1,182,720\n",
            "  3. Decoder Embedding (V_tgt * e):           67 * 256 =     17,152\n",
            "  4. Decoder GRU (n=1):                         1,182,720\n",
            "  5. Decoder Linear (h * V_tgt + V_tgt): (512 * 67) + 67 =     34,371\n",
            "  --------------------------------------------------\n",
            "  CALCULATED TOTAL:                         2,424,387\n",
            "\n",
            "--- Verification ---\n",
            "  - Actual Model Parameters:             2,424,387\n",
            "  - Verification:                  âœ… MATCH\n",
            "==================================================================\n",
            "\n",
            "\n",
            "==================================================================\n",
            "  Assignment's Theoretical Formulas (for 1-Layer GRU)\n",
            "==================================================================\n",
            "\n",
            "Q: What is the total number of parameters... (e, h, V, 1-layer)?\n",
            "A: V(2e + h + 1) + h(6e + 6h + 12)\n",
            "\n",
            "Q: What is the total number of computations... (e, h, V, T, 1-layer)?\n",
            "A: T * (6eh + 6h^2 + hV)\n",
            "==================================================================\n",
            "\n",
            "--- Interactive Transliteration (Hindi) ---\n",
            "Type a Romanized word and press Enter (or 'quit' to stop).\n",
            "ðŸ”¤ > ghar\n",
            "   â†’ à¤˜à¤°\n",
            "\n",
            "ðŸ”¤ > ajanabee\n",
            "   â†’ à¤…à¤œà¤¨à¤¬à¥€\n",
            "\n",
            "ðŸ”¤ > quit\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Install 'editdistance' for Character Error Rate (CER) ---\n",
        "# We use -q (quiet) to hide the installation logs\n",
        "print(\"Installing 'editdistance' for CER calculation...\")\n",
        "!pip install editdistance -q\n",
        "import editdistance\n",
        "import os\n",
        "print(\"Installation complete.\")\n",
        "\n",
        "# --- 2. Define the Dynamic Model Analyzer ---\n",
        "def analyze_and_print_report(model, embed_dim, hidden_dim, v_src, v_tgt):\n",
        "    \"\"\"\n",
        "    Analyzes this specific 1-layer GRU model and prints a detailed parameter report.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Set model-specific parameters ---\n",
        "    cell_type = \"GRU\"\n",
        "    num_layers = 1\n",
        "    gates = 3 # A GRU has 3 gates (reset, update, new)\n",
        "\n",
        "    # Aliases for easier formula reading\n",
        "    e, h, n = embed_dim, hidden_dim, num_layers\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 66)\n",
        "    print(\"      Dynamic Model Analysis (for 1-Layer GRU)\")\n",
        "    print(\"=\" * 66)\n",
        "    print(\"\\n--- Model Configuration ---\")\n",
        "    print(f\"  - Cell Type: {cell_type} (hardcoded in notebook)\")\n",
        "    print(f\"  - Num Layers (n): {n} (hardcoded in notebook)\")\n",
        "    print(f\"  - Embedding Dim (e): {e}\")\n",
        "    print(f\"  - Hidden Dim (h): {h}\")\n",
        "    print(f\"  - Source Vocab (V_src): {v_src}\")\n",
        "    print(f\"  - Target Vocab (V_tgt): {v_tgt}\")\n",
        "\n",
        "    print(\"\\n--- 1. Practical Parameter Calculation (from Config) ---\")\n",
        "\n",
        "    # --- Calculate Layer by Layer ---\n",
        "    p_enc_embed = v_src * e\n",
        "    p_dec_embed = v_tgt * e\n",
        "    p_dec_linear = (h * v_tgt) + v_tgt # (weights + bias)\n",
        "\n",
        "    # GRU Layer 1 (e -> h)\n",
        "    # Formula: gates * ( (e*h) + (h*h) + (bias_in) + (bias_hidden) )\n",
        "    rnn_l1_params = gates * (e * h + h * h) + 2 * (gates * h)\n",
        "\n",
        "    p_enc_rnn = rnn_l1_params\n",
        "    p_dec_rnn = rnn_l1_params\n",
        "\n",
        "    p_total_theoretical = p_enc_embed + p_enc_rnn + p_dec_embed + p_dec_rnn + p_dec_linear\n",
        "\n",
        "    # --- Get Actual Count from the model object ---\n",
        "    p_total_actual = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"  1. Encoder Embedding (V_src * e):      {v_src:7} * {e} = {p_enc_embed:10,}\")\n",
        "    print(f\"  2. Encoder GRU (n=1):                        {p_enc_rnn:10,}\")\n",
        "    print(f\"  3. Decoder Embedding (V_tgt * e):      {v_tgt:7} * {e} = {p_dec_embed:10,}\")\n",
        "    print(f\"  4. Decoder GRU (n=1):                        {p_dec_rnn:10,}\")\n",
        "    print(f\"  5. Decoder Linear (h * V_tgt + V_tgt): ({h} * {v_tgt}) + {v_tgt} = {p_dec_linear:10,}\")\n",
        "    print(\"  \" + \"-\" * 50)\n",
        "    print(f\"  CALCULATED TOTAL:                        {int(p_total_theoretical):10,}\")\n",
        "\n",
        "    print(\"\\n--- Verification ---\")\n",
        "    print(f\"  - Actual Model Parameters:         {p_total_actual:13,}\")\n",
        "    print(f\"  - Verification:                  {'âœ… MATCH' if p_total_actual == p_total_theoretical else 'âŒ MISMATCH'}\")\n",
        "    print(\"=\" * 66)\n",
        "\n",
        "    # --- 2. Print the Assignment's Formulas (for 1-layer GRU) ---\n",
        "    print(\"\\n\\n\" + \"=\" * 66)\n",
        "    print(\"  Assignment's Theoretical Formulas (for 1-Layer GRU)\")\n",
        "    print(\"=\" * 66)\n",
        "    print(\"\\nQ: What is the total number of parameters... (e, h, V, 1-layer)?\")\n",
        "    print(\"A: V(2e + h + 1) + h(6e + 6h + 12)\")\n",
        "\n",
        "    print(\"\\nQ: What is the total number of computations... (e, h, V, T, 1-layer)?\")\n",
        "    print(\"A: T * (6eh + 6h^2 + hV)\")\n",
        "    print(\"=\" * 66)\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# SCRIPT EXECUTION STARTS HERE\n",
        "# (This assumes 'model', 'src_to_ix', etc. exist from\n",
        "# running the previous cells in your notebook)\n",
        "# =============================================================\n",
        "\n",
        "# --- 3. Run Validation on Test Set ---\n",
        "print(\"\\nRunning validation on the test set...\")\n",
        "\n",
        "# Make sure DATA_ROOT is defined (it's in your training cell)\n",
        "DATA_ROOT = r\"W:\\CV\\HTIC\\Qns-3\\hin\"\n",
        "TEST_FILE = os.path.join(DATA_ROOT, \"hin_test.txt\")\n",
        "\n",
        "test_data = load_local_csv(TEST_FILE)\n",
        "if test_data:\n",
        "    # Filter test data\n",
        "    filtered_test_data = [\n",
        "        d for d in test_data\n",
        "        if all(c in src_to_ix for c in d[\"english word\"]) and \\\n",
        "           all(c in tgt_to_ix for c in d[\"native word\"])\n",
        "    ]\n",
        "    print(f\"Loaded {len(filtered_test_data)} test pairs (filtered from {len(test_data)}).\")\n",
        "\n",
        "    correct_count, total_edit_distance, total_target_chars = 0, 0, 0\n",
        "    total_count = len(filtered_test_data)\n",
        "\n",
        "    for pair in filtered_test_data:\n",
        "        # Use the 'transliterate_word' function defined in a previous cell\n",
        "        pred = transliterate_word(model, pair[\"english word\"], src_to_ix, tgt_to_ix, ix_to_tgt, DEVICE)\n",
        "        if pred == pair[\"native word\"]: correct_count += 1\n",
        "        total_edit_distance += editdistance.eval(pred, pair[\"native word\"])\n",
        "        total_target_chars += len(pair[\"native word\"])\n",
        "\n",
        "    print(\"\\n--- Test Set Results ---\")\n",
        "    print(f\"âœ… Exact Match Accuracy: {(correct_count / total_count) * 100:.2f}%\")\n",
        "    if total_target_chars > 0:\n",
        "        print(f\"ðŸ“Š Character Error Rate (CER): {(total_edit_distance / total_target_chars) * 100:.2f}%\")\n",
        "else:\n",
        "    print(\"Could not find test file. Skipping final validation.\")\n",
        "\n",
        "# --- 4. Save the Model ---\n",
        "MODEL_SAVE_PATH = os.path.join(DATA_ROOT, \"hin_model.pth\")\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"\\nâœ… Model weights saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "# --- 5. Call the Analyzer ---\n",
        "# We pass the variables that are in your notebook's global state\n",
        "analyze_and_print_report(\n",
        "    model=model,\n",
        "    embed_dim=EMBED_DIM,      # From your config cell\n",
        "    hidden_dim=HIDDEN_DIM,    # From your config cell\n",
        "    v_src=len(src_to_ix),     # From your training cell\n",
        "    v_tgt=len(tgt_to_ix)      # From your training cell\n",
        ")\n",
        "\n",
        "# --- 6. Start Interactive Loop ---\n",
        "print(\"\\n--- Interactive Transliteration (Hindi) ---\")\n",
        "print(\"Type a Romanized word and press Enter (or 'quit' to stop).\")\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"ðŸ”¤ > \").strip()\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "        if not user_input:\n",
        "            continue\n",
        "        # Use the 'transliterate_word' function defined in a previous cell\n",
        "        result = transliterate_word(model, user_input, src_to_ix, tgt_to_ix, ix_to_tgt, DEVICE)\n",
        "        print(f\"   â†’ {result}\\n\")\n",
        "    except EOFError:\n",
        "        # This handles the end of input in some notebook environments\n",
        "        print(\"Interactive loop finished.\")\n",
        "        break\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nInteractive loop stopped.\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "9MWRpASpTIaW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MWRpASpTIaW",
        "outputId": "59e6359f-844a-4282-f8b2-9807d9ed51bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating accuracy on the test set...\n",
            "Loaded 4094 test pairs (filtered from 4096).\n",
            "Running predictions...\n",
            "  Processed 500/4094 words...\n",
            "  Processed 1000/4094 words...\n",
            "  Processed 1500/4094 words...\n",
            "  Processed 2000/4094 words...\n",
            "  Processed 2500/4094 words...\n",
            "  Processed 3000/4094 words...\n",
            "  Processed 3500/4094 words...\n",
            "  Processed 4000/4094 words...\n",
            "\n",
            "--- Test Set Accuracy ---\n",
            "âœ… Exact Match Accuracy: 26.50% (1085 correct out of 4094)\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Calculate Accuracy on Test Set\n",
        "# ================================\n",
        "import os\n",
        "import torch\n",
        "# Make sure the following are defined from previous cells:\n",
        "# - model (your trained Seq2Seq model)\n",
        "# - src_to_ix, tgt_to_ix, ix_to_tgt (your vocab mappings)\n",
        "# - DEVICE (your torch.device)\n",
        "# - load_local_csv (your data loading function)\n",
        "# - transliterate_word (your prediction function)\n",
        "# - DATA_ROOT (path to your 'hin' folder)\n",
        "\n",
        "print(\"Calculating accuracy on the test set...\")\n",
        "\n",
        "# --- 1. Load Test Data ---\n",
        "TEST_FILE = os.path.join(DATA_ROOT, \"hin_test.txt\")\n",
        "test_data = load_local_csv(TEST_FILE)\n",
        "\n",
        "if test_data:\n",
        "    # --- 2. Filter Test Data ---\n",
        "    # Keep only words where all characters are known to the model\n",
        "    filtered_test_data = [\n",
        "        d for d in test_data\n",
        "        if all(c in src_to_ix for c in d[\"english word\"]) and \\\n",
        "           all(c in tgt_to_ix for c in d[\"native word\"])\n",
        "    ]\n",
        "    print(f\"Loaded {len(filtered_test_data)} test pairs (filtered from {len(test_data)}).\")\n",
        "\n",
        "    # --- 3. Initialize Counters ---\n",
        "    correct_count = 0\n",
        "    total_count = len(filtered_test_data)\n",
        "\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # --- 4. Iterate and Predict ---\n",
        "    print(\"Running predictions...\")\n",
        "    with torch.no_grad(): # Faster inference without gradient calculation\n",
        "        for i, pair in enumerate(filtered_test_data):\n",
        "            source_word = pair[\"english word\"]\n",
        "            target_word = pair[\"native word\"]\n",
        "\n",
        "            # Get the model's prediction\n",
        "            predicted_word = transliterate_word(\n",
        "                model, source_word, src_to_ix, tgt_to_ix, ix_to_tgt, DEVICE\n",
        "            )\n",
        "\n",
        "            # --- 5. Compare and Count ---\n",
        "            if predicted_word == target_word:\n",
        "                correct_count += 1\n",
        "\n",
        "            # Optional: Print progress\n",
        "            if (i + 1) % 500 == 0:\n",
        "                 print(f\"  Processed {i+1}/{total_count} words...\")\n",
        "\n",
        "    # --- 6. Calculate and Print Accuracy ---\n",
        "    if total_count > 0:\n",
        "        accuracy = (correct_count / total_count) * 100\n",
        "        print(\"\\n--- Test Set Accuracy ---\")\n",
        "        print(f\"âœ… Exact Match Accuracy: {accuracy:.2f}% ({correct_count} correct out of {total_count})\")\n",
        "    else:\n",
        "        print(\"No valid test data found after filtering.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Could not find or load test file: {TEST_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yHQA00-vlmeJ",
      "metadata": {
        "id": "yHQA00-vlmeJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
